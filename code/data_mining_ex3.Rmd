---
title: "ECO395M: Exercise 3"
author: "Kashaf Oneeb"
date: "4/1/2022"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1: What causes what?

## 1. Why can’t I just get data from a few different cities and run the regression of “Crime” on “Police” to understand how more cops in the streets affect crime? 

Firstly, grouping data from different cities makes the incorrect assumption that the data points are independently and identically distributed among cities. Crime rates and police measures vary among cities, and factors that influence these two variables, such as, police budgets, population, income levels, criminal punishments, etc. also vary amongst cities depending on varying state laws. As a result, combining data from different cities could distort the outcome of the regression. 
Secondly, while police measures affect crime rates, crime rates also affect the level of police measures, leading to a simultaneous causality bias. Thirdly, there is a lack of control variables in the regression as crime rates can be influenced by several other variables as mentioned above; this could lead to incorrect estimation of the effect of "Police" on "Crime".

## 2. How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers' paper.

The researchers from UPenn narrowed down their research to only one city, Washington, D.C., assuming that the data points would be identically distributed within a city. To deal with the simultaneous causality bias, they chose Washington, D.C. because it is more likely to be a terrorism target compared to other cities in the U.S. This allowed the researchers to utilize the "High Alert" variable as the instrument variable to address simultaneity. In other words, when there is High Alert (High Alert = 1), extra police is deployed in the city to protect against terrorists; in this setup the deployment of the extra police is not directly related to the variable of interest, "Crime". So, the High Alert variable is able to capture the effect of police measures on crime rates without crime rates affecting police measures. They also added the log of midday ridership on the Metro as a control variable to test whether there were fewer victims out in the city becuase of the High Alert.

Table 2 summarizes their results as two regressions. The first regression only utilizes the High Alert dummy variable to estimate the effect of police on the daily total number of crimes in D.C. This leads to -7.316 as the coefficient on High Alert which is significant at the 5% level. It means that on a High Alert day, the daily total number of crimes in D.C. by 7.316 units, holding all else constant. The R-squared for this regression is 0.14, which means only 14% of the variation in crime rates in D.C. is described by the covariate in the model.

The second regression adds the log of midday ridership to the first regression as a control variable. This leads to -6.046 as the coefficient on High Alert at the 5% significance level, meaning that on a High Alert day, the daily total number of crimes in D.C. falls by 6.046 units, holding all else constant. The coefficient on "Log(midday ridership)" is 17.341 which is significant at the 1% level. This means if midday ridership on the Metro increases by 1 percent, the daily total number of crimes in D.C. increase by 0.173 units, holding all else constant. The R-sqaured of this regression, 0.17, is slightly higher than that of the first regression. It means only 17% of the varation in crime rates in D.C. is described by the covariates in the model. Given that, since both of the coefficients in this regression are statistically significant at the 5% and 1% level, respectively, the second regression performs better than the first regression, although by only a small margin. 

## 3. Why did they have to control for Metro ridership? What was that trying to capture?

It is possible that on High Alert days, fewer people are out in the city due to higher threat levels of terrorism. Fewer people out in the city means fewer victims of crime which could lead to lower crime rates. Therefore, to account for this effect, Metro ridership needs to be controlled for. Metro ridership attempts to capture the number of potential victims of crime based on the changes in ridership with respect to High Alert days. 

## 4. Below I am showing you "Table 4" from the researchers' paper. Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?

The researchers are estimating the district-specific effects of High Alert days on the daily total number of crimes in D.C., while controlling for Metro ridership using a multiple regression approach with heteroscedastic errors. They introduce two district dummy variables, "District 1" and "Other Districts" and interact both the district dummy variables with the the High Alert variable to estimate district-specific effects. 
The coefficient on "High Alert x District 1" shows that on a High Alert day in District 1, the daily total number of crimes in D.C. falls by 13.679 units, holding all else constant. This coefficient is statistically significant at the 1% level. Similarly, the coefficient on "High Alert x Other Districts" shows that on a High Alert day in any other district, the daily total number of crimes in D.C. falls by 11.629 units, holding all else constant. These results reflect that a High Alert day in District 1 reduces crime rates more than a High Alert day in any other district. However, the coefficent on "High Alert x Other Districts" is not statistically significant. Lastly, the coefficient on "Log (midday ridership)" reflects that a 1 percent increase in midday ridership on the Metro increases the daily total number of crimes in D.C. 2.477 units, holding all else constant. This coefficient is statistically significant at the 5% level. 

Given the statisical significance of the coefficients, it can be concluded that having a High Alert day in District 1 reduces the daily total number of crimes in D.C. by 13.679 units, holding all else constant. However, a comparison between District 1 and Other Districts cannot be made due the lack of significance of the "High Alert x Other Districts" coefficient.

# Problem 2: Tree modeling: dengue cases

## CART model: Regression Tree

Below is an unpruned regression tree that utilizes the following variables: ***cityseason***, ***specific_humidity***, ***tdtr_k***, and ***precipitation_amt*** to predict the total dengue cases, ***total_cases***.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
library(here)
library(tidyverse)
library(rsample)
library(modelr)
library(rpart)
library(rpart.plot)

here::i_am("code/data_mining_ex3.Rmd")

# read in the data
dengue <- read.csv(here("data/dengue.csv"))

# exclude missing values
dengue <- na.exclude(dengue)

# convert chr type to factor type
dengue <- as.data.frame(unclass(dengue),              
                       stringsAsFactors = TRUE)

# split the data into training and test sets
dengue_split = initial_split(dengue, prop = 0.8)
dengue_train = training(dengue_split)
dengue_test = testing(dengue_split)

# CART model: Regression Tree

# fit the rpart model on the training set
rpart_fit <- rpart(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt, 
  data = dengue_train,
  control=rpart.control(cp=.0001, minsplit=20))

# plot the regression tree
rpart.plot(rpart_fit, digits=-5, type=4, extra=1)
```


### Pruned regression tree at the 1 standard error complexity level

Below is a pruned regression tree that uses the same variables as the tree above at the 1 standard error complexity level to predict total dengue cases.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
# function to prune a tree at the 1se level
prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

# prune rpart_fit at the 1se complexity level
rpart_fit_prune = prune_1se(rpart_fit)

# plot the pruned tree
rpart.plot(rpart_fit_prune, digits=-5, type=4, extra=1)
```

### RMSE of predicting the regression tree on the test set

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

rmse(rpart_fit_prune, dengue_test)

```

## Random forest

The random forest model also utilizes the same variables to predict the total dengue cases.

###  RMSE of predicting random forest on the test set

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

library(randomForest)

# fit a random forest model on the train set
random_forest_fit = randomForest(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
                           data=dengue_train, importance = TRUE)

rmse(random_forest_fit, dengue_test)

```

## Gradient-Boosted Trees

The gradient boosted model also utilizes the same variables to predict the total dengue cases. I first built a model assuming Gaussian distribution and then built another model assuming Poisson distribution.

### Gaussian distribution

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
library(gbm)
library(caret)

# build boosted regression trees with gaussian distribution and 10-fold cv
boost1 <- gbm(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt, 
               data = dengue_train,
               distribution = "gaussian",
               cv.folds = 10,
               interaction.depth=4, n.trees=500, shrinkage=.05)

# plot loss function as a result of n trees added to the ensemble
gbm.perf(boost1, method = "cv")

```

The figure above displays a loss function as a result of n trees added to the ensemble for the chosen gradient boosted machine (gbm) model . The model assumes Gaussian distibution with 500 trees. The squared error loss is shown on the y-axis and numbers of trees is on the x-axis. It can be seen that the cross-validated errors represented by the green line are minimized between 40 to 90 trees. Therefore, the ideal number of trees to minimize error while accounting for overfitting would be 100. The number of trees that minimizes the squared error loss in this run of the model is also displayed above.

The RMSE between the predicted total number cases of dengue from the chosen gbm model and the actual total number of cases of dengue from the test set is shown below.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# predict boost1 on test set to obtain yhat
yhat_test_gbm = predict(boost1, dengue_test, n.trees=100)

# RMSE between yhat and y from the test set
RMSE(yhat_test_gbm , dengue_test$total_cases)

```

### Poisson distribution

This gbm model assumes poisson distibution since the variable of interest is a count variable. All parameters are the same as for the model shown above, except the number of trees is 100 based on the loss function above.
The RMSE between the predicted total number cases of dengue from the chosen gbm model and the actual total number of cases of dengue from the test set is shown below.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# build boosted regression trees with poisson distribution and 100 trees
boost2 <- gbm(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
                data = dengue_train,
                distribution="poisson",
                cv.folds = 10,
                interaction.depth=4, n.trees=100, shrinkage=.05)

# predict boost2 on test set to obtain yhat
yhat_test_gbm2 <- predict(boost2, dengue_test, n.trees=100, type="response")

# RMSE between yhat and y from the test set
RMSE(yhat_test_gbm2,dengue_test$total_cases)

```

Since, the RMSE for the random forest model is the lowest out of all the models observed above, it will be utilized to create partial dependence plots for the following variables: specific_humidity, precipitation_amt, and tdrk_k.

## Partial dependence plots

### specific_humidity

The average specific humidity in grams of water per kilogram of air for the week. This is a raw measure of humidity based purely on how much water is in the air.

It can be seen from the plot below that as the average specific humidity rises, the predicted number of dengue cases also rises until it flatten outs as the average specific humidity approaches 19 grams of water per kilogram of air for the week.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
partialPlot(random_forest_fit, dengue, "specific_humidity", las=1)
```

### precipitation_amt

Rainfall for the week in millimeters.

It can be seen from the plot below that the relationship between predicted dengue cases and precipitation is quite volatile until precipitation reaches 100 millimeters. Beyond 100 millimeters of precipitation, there seems to be a positive relationship between dengue cases and precipitation until it becomes levels off beyond 290 millimeters of precipitation.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
partialPlot(random_forest_fit, dengue, "precipitation_amt", las=1)
```

### tdtr_k

The average Diurnal Temperature Range (DTR) for the week. DTR is the difference between the maximum and minimum temperature for a single day.

It can be seen from the plot below that there is a negative relationship between dengue cases and DTR until DTR reaches approximately 7. Beyond that point, the dengue cases remain relatively constant as DTR increases. 

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
partialPlot(random_forest_fit, dengue, "tdtr_k", las=1)
```

# Problem 3: Predictive model building: green certification

## Goal

The goal is to build the best predictive model possible for revenue per square foot per calendar year, and to use this model to quantify the average change in rental income per square foot associated with green certification, holding other features of the building constant.

## Data Pre-processing

I first created a new data frame with a ***revenue*** column which is a product of the ***Rent*** and ***leasing_rate***/100 variables. Since ***leasing_rate*** is a percentage, I divided it by 100 to only get the factor effect. I also removed ***Rent*** and ***leasing_rate*** from the new data frame to prevent multicollinearity and eliminate the need to remove these two variables from the models specified below. As a final pre-processing step, I excluded any missing values in the dataset and split 80% of the data into the training set and 20% of the data into the test set. 
Note: I did not scale the data since the modeling approaches I will be pursuing do not involve distances, hence they are not sensitive to the variance in the data.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# read in the data
green0 <- read.csv(here("data/greenbuildings.csv"))

# create new dataframe with a revenue column and remove rent and leasing rate
green <- mutate(green0,
                revenue = Rent*(leasing_rate/100))
green <- select(green, -Rent, -leasing_rate)

# exclude missing values
green <- na.exclude(green)


```

## Modeling approach

### Linear Model (Baseline)

In order to have a baseline to compare other models to, I performed a multiple regression on all the features except ***total_dd_07***, and ***green_rating*** (to avoid multicollinearity) in the training set. The RMSE of predicting the linear model on the test set is shown below.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
library(mosaic)

# split the data into training and test sets
green_split = initial_split(green, prop = 0.8)
green_train = training(green_split)
green_test = testing(green_split)

# fit a linear model to the training set
green_lm <- lm(revenue ~ . - total_dd_07 - green_rating, 
                  data=green_train)


rmse_sim = do(10)*{
  green_split = initial_split(green, prop = 0.8)
  green_train = training(green_split)
  green_test = testing(green_split)
  
  green_lm = update(green_lm, data=green_train)
  

  model_errors = c(rmse(green_lm, green_test))
  
}

# Average performance across the splits
 rmse_means_lm <- colMeans(rmse_sim)
 rmse_means_lm
```

### Random forest (Model of Choice)

I chose the random forest technique as it is quite robust, quick, and basically eliminates the need for cross-validation due to its "out-of-bag" predictions. I utilized all of the features in the training set except ***total_dd_07***, and ***green_rating*** (to avoid multicollinearity) and calculated the resulting RMSE between the predcited revenue from the model and the actual revenue from the test set which is shown below. It can be seen that the random forest model performs better than the linear model based on the RMSE difference between the two.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# fit a random forest model on the training set
green_random_forest_fit = randomForest(revenue ~ . - total_dd_07 - green_rating,
                           data=green_train, importance = TRUE)
# calculate rmse of the model on the test set
rmse(green_random_forest_fit, green_test)
```

To identify the important variables in the random forest model, I built the variable importance plot shown below. It can be seen that ***hd_total07*** ***LEED***, ***Energystar***, and ***net*** are the least important variables, in that order (decreasing in importance). 

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
vi = varImpPlot(green_random_forest_fit, type=1)
```

Since ***net*** is the least important variable, I attempted to further improve the current random forest model by removing it. The RMSE of the new model is shown below. Based on the RMSE, this model does not necessarily perform better than the previous random forest model; however, it still outperforms the multiple regression model.


```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
# fit a random forest model on the training set
green_random_forest_fit1 = randomForest(revenue ~ . - total_dd_07 - green_rating - net,
                           data=green_train, importance = TRUE)

# calculate rmse of the model on the test set
rmse(green_random_forest_fit1, green_test)
```

To quantify the average change in revenue per square foot I decided to build partial dependence plots (pdp) associated with green certifications using the first random forest model. The pdp's for the ***LEED*** and ***Energystar*** variables are shown below.


```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

partialPlot(green_random_forest_fit, green, "LEED", las=1)
partialPlot(green_random_forest_fit, green, "Energystar", las=1)
```

## Conclusion

The pdp for LEED shows that when LEED = 0, the predicted revenue per square foot per calendar year is slightly over 24 and when LEED = 1, the predicted revenue per square foot per calendar year is approximately 26. Likewise, the pdp for Energystar shows that when Energystar = 0, the revenue per square foot per calendar year is slightly above 24.05 and when Energystar = 1, the revenue per square foot per calendar year is approximately 24.4. 

These plots reflect that although the revenue predictions for buildings with green ratings are higher than buildings without green ratings, they are higher by only a small margin. For instance, buildings with a LEED certification are predicted to only make ~2 dollars more in revenue on average than buildings without a LEED certification. Likewise, buildings with an Energystar certification are predicted to only make ~35 cents more in revenue on average than buildings without an Energystar certification.

Hence, it can be concluded that energy certifications do not influence building revenue per square foot per calendar year very significantly. 

# Problem 4:Predictive model building: California housing

## Goal

The goal is to build the best predictive model for median house values in California.

## Data Pre-processing

I created a new data frame with standardized versions of ***totalRooms*** and ***totalBedrooms*** obtained by dividing the two variables by the ***households*** variable to get averages instead of totals. Additionally, I removed ***totalRooms*** and ***totalBedrooms*** from the new data frame to avoid multicollinearity and save writing in model specification. Lastly, I excluded any missing values and split 80% of the data into the training set and 20% of the data into the test set. 
Note: I did not scale the data since the modeling approaches I will be pursuing do not involve distances, hence they are not sensitive to the variance in the data.


```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# read in the data
ca_housing0 <- read.csv(here("data/CAhousing.csv"))

# standardize totalRooms and totalBedrooms by households
ca_housing <- mutate(ca_housing0,
                avgRooms = (totalRooms/households),
                avgBedrooms = (totalBedrooms/households))

# remove unstandardized totalRooms and totalBedrooms
ca_housing <- select(ca_housing, -totalRooms, -totalBedrooms)

# exclude missing values
green <- na.exclude(ca_housing)


ca_housing_split = initial_split(ca_housing, prop = 0.8)
ca_housing_train = training(ca_housing_split)
ca_housing_test = testing(ca_housing_split)
  

```

## Modeling Approach

### Linear Model (Baseline)

In order to have a baseline to compare other models to, I performed a multiple regression on all the features in the training set. The RMSE of predicting the linear model on the test set is shown below.

````{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

ca_housing_lm <- lm(medianHouseValue ~ . , data=ca_housing_train)

# fit a linear model to the training set
ca_housing_rmse_sim = do(10)*{
  ca_housing_split = initial_split(ca_housing, prop = 0.8)
  ca_housing_train = training(ca_housing_split)
  ca_housing_test = testing(ca_housing_split)

  ca_housing_lm = update(ca_housing_lm, data=ca_housing_train)
  

  ca_housing_model_errors = c(rmse(ca_housing_lm, ca_housing_test))
  
}

# Average performance across the splits
 ca_housing_rmse_means_lm <- colMeans(ca_housing_rmse_sim)
 ca_housing_rmse_means_lm
```

### Random Forest

I utilized all of the features in the training set and calculated the resulting RMSE between the predcited revenue from the random forest model and the actual revenue from the test set which is shown below. It can be seen that the random forest model significantly outperforms the linear model based on the RMSE difference between the two (~18000).


```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# fit a random forest model on the training set
ca_housing_random_forest_fit = randomForest(medianHouseValue ~ .,
                           data=ca_housing_train, importance = TRUE)

# calculate rmse of the model on the test set
rmse(ca_housing_random_forest_fit, ca_housing_test)
```
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
vi1 = varImpPlot(ca_housing_random_forest_fit, type=1)
```

### Gradient-Boosted Trees (Model of Choice)

I used all of the features in the training set and assumed a Gaussian distibution for the gradient boosted model with 10-fold cross-validation. The parameters for this model are as follows:  interaction.depth = 4, n.trees = 3000, shrinkage = .05. I ended up choosing such a high number of trees becuase the loss function kept showing the n-th tree as the mean-squared-error minimizer until I chose 3,000 and received a 2,900+ number as the mean-squared-error minimizer. The number of trees that minimized the mean-squared-error and a plot of the loss function is displayed below. Additionally, I manually cross-validated the interaction depth and shrinkage based on the lowest RMSE measure and arrived at the chosen parameters. 

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# build boosted regression trees with gaussian distribution and 10-fold cv
ca_boost <- gbm(medianHouseValue ~ ., 
               data = ca_housing_train,
               distribution = "gaussian",
               cv.folds = 10,
               interaction.depth=4, n.trees=3000, shrinkage=.05)

# plot loss function as a result of n trees added to the ensemble
gbm.perf(ca_boost, method = "cv")

```

The RMSE between the predicted revenue and the actual revenue from the test set is displayed below. It is lower than that of the random forest model. Therefore, this model will be the model of choice.


```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# predict boost on test set to obtain yhat
ca_yhat_test_gbm = predict(ca_boost, ca_housing_test, n.trees=3000)

# RMSE between yhat and y from the test set
RMSE(ca_yhat_test_gbm , ca_housing_test$medianHouseValue)

```

## Map Plots

### Actual Median House Values

The distribution of the actual median house values is shown below on the California map. The values increase from green to red on the color scale. 

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# plot the original data, using a color scale to show medianHouseValue versus longitude (x) and latitude (y).

which_state <- "california"
county_info <- map_data("county", region=which_state)

base_map <- ggplot(data = county_info, mapping = aes(x = long, y = lat, group = group)) +
 geom_polygon(color = "black", fill = "white") +
  coord_quickmap() +
  theme_void() 

data_map <- base_map +
  geom_point(data = ca_housing, aes(x=longitude, y=latitude, color=medianHouseValue, group=medianHouseValue)) +
  scale_color_gradient(low="green", high="red") +
  labs(title = str_wrap("A scatterplot of the actual Median House Values in California from the data set", 60),
       color = "Median House Values") 


data_map
```

### Predicted Median House Values

The distribution of the gbm predicted median house values is shown below on the California map. The values increase from green to red on the color scale. The color distribution is quite similiar to that of the actual values, with the exception of high values. It seems that my model did not predict the high values as high as they should have been since mostly orange data points are observed in place of red ones. 

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

pred_ca_housing <- ca_housing %>%
  modelr::add_predictions(ca_boost)

# plot gbm's predictions of medianHouseValue versus longitude (x) and latitude (y)
pred_map <- base_map +
  geom_point(data = pred_ca_housing, aes(x=longitude, y=latitude, color=pred, group=medianHouseValue)) +
  scale_color_gradient(low="green", high="red") +
  labs(title = str_wrap("A scatterplot of predicted Median House Values in California using the gradient boosting model", 60),
              color = "Predicted Median House Values")

pred_map

```

### Residuals

The distribution of the residuals from the gbm model is shown below on the California map. The resisduals were calculated as differenced between predicted median house values and actual median house values. Blue-ish points represent negative differences, green points represent minimal to no differences, and red-ish points represent positive differences. It can be seen that most of the data points are in the green range, which reflects the success of the gbm model in predicting median house values. 

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

pred_ca_housing <- pred_ca_housing %>%
  mutate(residuals=(pred-medianHouseValue))

# plot gbm's predictions of medianHouseValue versus longitude (x) and latitude (y)
pred_map <- base_map +
  geom_point(data = pred_ca_housing, aes(x=longitude, y=latitude, color=residuals, group=medianHouseValue)) +
  scale_color_gradient2(low="blue", mid="green", high="red") +
  labs(title = str_wrap("A scatterplot of residuals from the gradient boosting model", 60),
              color = "Residuals")

pred_map

```
